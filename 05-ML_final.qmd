---
title: "Spillover Effects of Platform Entry: Predicting Third-Party Seller Exit on JD.com"
author: "Zohre Yahyaee"
date: today
format: pdf
bibliography: references.bib
csl: apa.csl
abstract: |
  This study investigates whether first-party (1P) entry into product spaces on JD.com increases the likelihood of third-party (3P) SKU exit. Using SKU-level panel data and a range of classification models, we find that exit appears to occur largely at random, which limits the predictive power of all tested models. Nevertheless, we observe that features capturing lack of recent orders and user interaction provide some explanatory value. Our best-performing model, an XGBoost classifier tuned via Bayesian optimization and interpreted using SHAP values, identifies disengagement and product space crowding as important—but ultimately insufficient—signals for accurately predicting exit behavior.
---

```{r theme, include=FALSE,echo = FALSE, message = FALSE}
library(ggplot2)  # <- make sure this is loaded before theme_set()

theme_set(theme_minimal(base_size = 12))
theme_update(
  plot.title = element_text(hjust = 0.5, face = "bold"),
  axis.title = element_text(face = "bold")
)


```

# Introduction

## 1.1 What is the research question?

In this research, we investigate the dynamics of hybrid e-commerce platforms, where the platform plays a dual role: acting both as a retailer by selling its own products and as a platform operator facilitating third-party sellers. This dual role can create a conflict of interest, as the platform both supports and competes with third-party sellers. Prior research suggests that the entry of the platform as a first-party seller can negatively impact third-party sellers, potentially leading some of them to exit the market @zhu2018competing; @jiang2011firm. Our goal is to empirically examine this phenomenon and understand how the platform’s dual role influences third-party seller behavior.

## 1.2. Why is this research question important?

This question is important because the hybrid platform business model has become ubiquitous among online platforms globally. For example, Amazon in the U.S. has utilized this model since the early 2000s, while Alibaba and JD.com in China also follow similar practices @hu2024supercharged; @wells2018amazon. Understanding the impact of this model is crucial for both policymakers and businesses, as it shapes market competition and affects seller sustainability. While hybrid platforms offer benefits through increased consumer reach and operational efficiency, they also raise concerns about fair competition and platform bias—particularly when the platform uses its data or visibility advantages to undermine independent sellers @crawford2022amazon; @he2020impact.

[^1]

[^1]: I thank Professor Eric Weisbrod for guidance and patience for this challenging project. This project was completed for ACCT 995 at the University of Kansas.

## 1.3. What is the variable of interest?

The main variable of interest is a binary classification outcome indicating whether a third-party SKU deactivated (i.e., exited) within the observed time period. This allows us to analyze the likelihood and predictors of seller exit from the platform.

# Literature Review

The rise of hybrid platforms—where the platform competes directly with the sellers it hosts—has raised concerns about fair competition and self-preferencing behavior. Foundational theoretical and empirical work by @jiang2011firm and @zhu2018competing suggests that platform entry is often strategic, targeting successful product spaces based on third-party seller performance. While this allows platforms to reduce uncertainty, it also creates potential harm: platform entry is associated with reduced sales, lower visibility, and even exit for affected third-party sellers. These findings support the view that platform entry can crowd out independent merchants, though the magnitude and mechanisms of these effects may vary across product types.

On one hand, @zhu2018competing show that Amazon's entry reduces third-party sales and discourages continued participation. Similarly, @song2020spillover document that sellers anticipate platform entry and adjust their product variety accordingly. These studies highlight the competitive threat that platform entry can pose.

On the other hand, @deng2023can and @chi2022competition present evidence for positive spillovers: when the platform enters a product space, it may boost overall demand through marketing and consumer awareness, indirectly benefiting existing third-party sellers. In some cases, third-party sales even rise after platform entry.

The divergence in these findings points to the role of context—such as product popularity, brand strength, and timing of entry. Some products may benefit from visibility spillovers, while others suffer from direct substitution. However, few studies systematically examine whether such spillover effects translate into exit behavior. In this project, we shift the focus from pricing and sales to the more fundamental question of market survival. By predicting third-party exit, we offer new insight into how platform dynamics shape seller persistence under competitive pressure.

```{r setup, include=FALSE,echo = FALSE, message = FALSE}
#library(tidymodels)
library(themis)
library(dplyr)
library(ggplot2)
library(lubridate)
library(vip)
library(DALEXtra)
library(workflowsets)
library(doParallel)
library(glue)
library(patchwork)  # for combining plots
library(forcats)
library(dplyr)
library(ggplot2)
library(yardstick)
library(tune)
library(workflows)
library(rsample )
# Define your metrics
library(yardstick)
library(doFuture)

library(tune)
library(finetune)
library(parsnip)
library(workflows)
library(yardstick)
library(dplyr)
library(dials)
library(yardstick)
library(dplyr)
library(vip)
registerDoFuture()
plan(multisession, workers = parallel::detectCores() - 1)

```

```{r Load data, include=FALSE,echo = FALSE, message = FALSE}

source("00_setup.R")

df <-read_csv(glue("{data_path}/balanced_panel_with_clicks_and_order_sku.csv"))

# Preview
head(df)
summary(df)

```

#Dataset Overview \## 3.1. What is the dataset about?

This study draws on SKU-level transaction data from JD.com, a major hybrid e-commerce platform in China. The dataset spans a single month—March 2018—and provides detailed information about each SKU’s commercial activity and platform presence. For each SKU, the data includes:

-   Product characteristics (e.g., brand, functional attributes),
-   Activation and deactivation dates (capturing SKU entry and exit),
-   User interaction measures (e.g., daily clicks, views), and
-   Purchase activity (e.g., daily order counts).

An important feature of this dataset is that JD.com maintains separate listings for each seller-SKU pair, allowing for precise tracking of SKU engagement over time. This structure provides a clean empirical setting to analyze third-party (3P) seller behavior—particularly decisions to deactivate a SKU—amid potential competition from platform-backed first-party (1P) sellers @shen2024jd.

## 3.2. What are the dataset challenges?

This dataset offers rich transactional data, but several limitations complicate the analysis:

-   **Short observation window**: The data covers a single month (March 2018), which restricts our ability to observe long-term exit patterns or delayed responses to platform entry.
-   **Incomplete attribute data**: Many SKUs lack complete information on core product characteristics, making it difficult to consistently group them into functionally comparable product spaces. As a result, we focus our analysis on the subset of SKUs with non-missing values for both functional attributes and brand identifiers.
-   **Defining true entry events**: JD.com’s structure makes it challenging to isolate clean instances of first-party entry. We mitigate this by identifying product spaces that transitioned from exclusive third-party to joint presence with first-party SKUs during the sample period @shen2024jd.

## 3.3. Exploratory Data Analysis (EDA)

We begin with a descriptive overview of the dataset:

-   The dataset includes 31,876 unique SKUs, each linked to a seller type, brand, and product attributes.
-   Over 20 million user click events and approximately 500,000 purchase transactions are recorded, capturing detailed consumer interactions @shen2024jd.
-   Despite the scale, only 9,159 SKUs received at least one order during the observation window, highlighting the skewed nature of demand @ernst2006estimating.
-   Preliminary comparisons reveal sharp contrasts between seller types:\
    First-party SKUs received an average of 223 orders, while third-party SKUs averaged only 9 orders.

These disparities underscore the need to examine whether platform entry is associated with reduced visibility or demand for third-party sellers—raising the question of whether first-party participation displaces existing market participants @zhu2018competing; @deng2023can.

```{r Filter to 3P SKUs and define target variable, include=FALSE,echo = FALSE, message = FALSE}

# Step 1: Filter to 3P SKUs and define target variable
sku_data <- df %>%
  filter(type == 2) %>%
  mutate(
    exited = factor(!is.na(deactivate_date), levels = c(FALSE, TRUE), labels = c("active", "exited"))
  )

# Step 2: Make sure categorical variables are explicit
sku_data <- sku_data %>%
  mutate(
    attribute1 = forcats::fct_explicit_na(as.factor(attribute1), na_level = "Missing"),
    attribute2 = forcats::fct_explicit_na(as.factor(attribute2), na_level = "Missing")
  )

# Quick check
table(sku_data$exited)
```

```{r Interaction streak and price quantiles, include=FALSE,echo = FALSE, message = FALSE }


# Step 1: Create date-based indicators
sku_data <- sku_data %>%
  mutate(
    activate_day = day(as.POSIXct(activate_date, origin = "1970-01-01")),
    deactivate_day = day(as.POSIXct(deactivate_date, origin = "1970-01-01")),
    is_interacting = NO_sales_within_day > 0 | number_of_unique_users_clicked_daily > 0
  )

# Step 2: Create interaction streak
sku_data <- sku_data %>%
  mutate(in_active_window = is.na(deactivate_day) | day_of_month <= deactivate_day) %>%
  arrange(sku_ID, day_of_month) %>%
  group_by(sku_ID) %>%
  mutate(
    non_interacting_streak = {
      streak <- numeric(n())
      for (i in seq_along(is_interacting)) {
        if (i == 1) {
          streak[i] <- ifelse(!is_interacting[i] & in_active_window[i], 1, 0)
        } else {
          streak[i] <- ifelse(!is_interacting[i] & in_active_window[i], streak[i-1] + 1, 0)
        }
      }
      streak
    }
  ) %>%
  ungroup() %>%
  group_by(sku_ID) %>%
  mutate(max_non_interacting_streak = max(non_interacting_streak)) %>%
  ungroup() %>%
  select(-in_active_window)

# Step 3: Price quantiles
priced_skus <- sku_data %>%
  filter(!is.na(imputed_ave_final_unit_price)) %>%
  group_by(product_space) %>%
  mutate(price_quantile_product = ntile(imputed_ave_final_unit_price, 10)) %>%
  ungroup() %>%
  group_by(brand_ID) %>%
  mutate(price_quantile_brand = ntile(imputed_ave_final_unit_price, 10)) %>%
  ungroup() %>%
  mutate(price_quantile_overall = ntile(imputed_ave_final_unit_price, 10))

# Step 4: Merge quantiles back
price_quantiles <- priced_skus %>%
  group_by(sku_ID) %>%
  summarise(
    price_quantile_product = first(price_quantile_product),
    price_quantile_brand = first(price_quantile_brand),
    price_quantile_overall = first(price_quantile_overall)
  )

sku_data <- sku_data %>%
  left_join(price_quantiles, by = "sku_ID")
```

```{r order-click-quantiles, include=FALSE,echo = FALSE, message = FALSE}
# Step 1: Summarize orders and clicks at the product space level
agg_stats <- sku_data %>%
  group_by(product_space) %>%
  summarise(
    total_orders_product_space = sum(NO_sales_within_day, na.rm = TRUE),
    total_clicks_product_space = sum(click_including_refresh_daily, na.rm = TRUE)
  ) %>%
  ungroup()

# Step 2: Join these back to SKU-level data
sku_data <- sku_data %>%
  left_join(agg_stats, by = "product_space")

# Step 3: Compute quantiles for orders
order_quantiles <- sku_data %>%
  filter(!is.na(total_orders_product_space)) %>%
  group_by(product_space) %>%
  mutate(order_quantile_product = ntile(total_orders_product_space, 10)) %>%
  ungroup() %>%
  group_by(brand_ID) %>%
  mutate(order_quantile_brand = ntile(total_orders_product_space, 10)) %>%
  ungroup() %>%
  mutate(order_quantile_overall = ntile(total_orders_product_space, 10))

# Step 4: Compute quantiles for clicks
click_quantiles <- order_quantiles %>%
  filter(!is.na(total_clicks_product_space)) %>%
  group_by(product_space) %>%
  mutate(click_quantile_product = ntile(total_clicks_product_space, 10)) %>%
  ungroup() %>%
  group_by(brand_ID) %>%
  mutate(click_quantile_brand = ntile(total_clicks_product_space, 10)) %>%
  ungroup() %>%
  mutate(click_quantile_overall = ntile(total_clicks_product_space, 10))

# Step 5: Reduce to one row per SKU and merge back
click_order_summary <- click_quantiles %>%
  group_by(sku_ID) %>%
  summarise(
    order_quantile_product = first(order_quantile_product),
    order_quantile_brand = first(order_quantile_brand),
    order_quantile_overall = first(order_quantile_overall),
    click_quantile_product = first(click_quantile_product),
    click_quantile_brand = first(click_quantile_brand),
    click_quantile_overall = first(click_quantile_overall)
  )

sku_data <- sku_data %>%
  left_join(click_order_summary, by = "sku_ID")
```

```{r Aggregate to SKU level,echo = FALSE, message = FALSE}

# Define variables to keep
static_vars <- c(
  "attribute1", "attribute2", "brand_ID", "product_space",
  "enter_typ_one", "NO_type_2_activated_in_product_space",
  "total_sku_type_1_in_product_space", "total_sku_type_2_in_product_space", "total_sku_in_product_space",
  "enter_typ_one_within_brand", "NO_type_1_activated_in_product_spacebrand_ID",
  "NO_type_2_activated_in_product_spacebrand_ID", "total_sku_type_1_in_product_spacebrand_ID",
  "total_sku_type_2_in_product_spacebrand_ID", "total_sku_in_product_spacebrand_ID",
  "max_non_interacting_streak", "exited",
  "price_quantile_product", "price_quantile_brand", "price_quantile_overall",
  "order_quantile_product", "order_quantile_brand", "order_quantile_overall",
  "click_quantile_product", "click_quantile_brand", "click_quantile_overall"
)

# Aggregate to SKU level
controlvariables <- sku_data %>%
  group_by(sku_ID) %>%
  summarise(across(all_of(static_vars), ~ first(.x))) %>%
  ungroup()



# Quick structure check
#glimpse(controlvariables)
table(controlvariables$exited)

```

```{r Define predictor variables,include=FALSE,echo = FALSE, message = FALSE}

ml_predictors <- c(
  "enter_typ_one",
  "total_sku_type_1_in_product_space",
  "total_sku_type_2_in_product_space",
  "total_sku_in_product_space",
  "price_quantile_overall",
  "price_quantile_product",
  "price_quantile_brand",
  "order_quantile_product",
  "order_quantile_brand",
  "order_quantile_overall",
  "click_quantile_product",
  "click_quantile_brand",
  "click_quantile_overall",
  "attribute1", 
  "attribute2",
  "max_non_interacting_streak",
  "total_sku_type_1_in_product_spacebrand_ID",
  "total_sku_type_2_in_product_spacebrand_ID",
  "NO_type_2_activated_in_product_spacebrand_ID",
  "NO_type_1_activated_in_product_spacebrand_ID"
)

```

## 3.4 Feature Engineering

To support predictive modeling, we engineered a set of variables that reflect three core aspects of SKU behavior: the level of competition in the product space, product characteristics, and indicators of platform performance. The following paragraphs describe the purpose and construction of each group of features.

-   The feature `max_non_interacting_streak` was created to capture disengagement. This variable records the maximum number of consecutive days during which a SKU received neither a click nor an order. It serves as a proxy for inactivity and is intended to reflect declining customer interest or visibility over time.

-   To control for variation in scale across categories, we constructed quantile-based rankings for prices, orders, and clicks. These were computed separately at three levels: within product space, within brand, and across the entire catalog. The resulting variables include price quantile within product space (`price_quantile_product`), click quantile within brand (`click_quantile_brand`), and others. These features help capture a SKU's relative position in terms of price and demand.

-   Several features were included to measure competition within a SKU’s environment. These include the total number of SKUs in the product space, the number of first-party and third-party SKUs within the same brand and product group, and whether a first-party SKU entered the space. Together, these variables are intended to capture the intensity of local competition and potential crowding effects.

-   Product-level attributes were retained as categorical variables. The fields `attribute1` and `attribute2` represent the functional classification of each SKU. To avoid dropping observations with incomplete data, missing values were explicitly labeled as "Missing" and treated as a separate category.

Table \@ref(tab:variable-description-table) presents the key variables used in the prediction model. These variables span three conceptual groups: (1) competitive pressure, such as the number of rival SKUs in a product space; (2) inherent product or brand characteristics, including categorical attributes and brand identity; and (3) platform performance indicators, such as click volume, order quantiles, and recent interaction history.

```{r var-description-table, message=FALSE, warning=FALSE}
library(knitr)
library(tibble)
library(kableExtra)

var_descriptions <- tibble::tibble(
  Variable = c(
    "enter_typ_one",
    "total_sku_type_1_in_product_space",
    "total_sku_type_2_in_product_space",
    "total_sku_in_product_space",
    "total_sku_type_1_in_product_spacebrand_ID",
    "total_sku_type_2_in_product_spacebrand_ID",
    "NO_type_2_activated_in_product_spacebrand_ID",
    "NO_type_1_activated_in_product_spacebrand_ID",
    "price_quantile_overall",
    "price_quantile_product",
    "price_quantile_brand",
    "order_quantile_product",
    "order_quantile_brand",
    "order_quantile_overall",
    "click_quantile_product",
    "click_quantile_brand",
    "click_quantile_overall",
    "max_non_interacting_streak",
    "attribute1",
    "attribute2"
    
  ),
  Description = c(
    "Indicates whether a 1P SKU entered the product space.",
    "Number of 1P SKUs in the product space.",
    "Number of 3P SKUs in the product space.",
    "Total number of SKUs in the product space.",
    "Number of 1P SKUs in product space within brand.",
    "Number of 3P SKUs in product space within brand.",
    "Number of newly activated 3P SKUs in brand-product space.",
    "Number of newly activated 1P SKUs in brand-product space.",
    "SKU's price quantile across all SKUs.",
    "SKU's price quantile within product space.",
    "SKU's price quantile within brand.",
    "SKU's order quantile within product space.",
    "SKU's order quantile within brand.",
    "SKU's order quantile overall.",
    "SKU's click quantile within product space.",
    "SKU's click quantile within brand.",
    "SKU's click quantile overall.",
    "Max consecutive days with no interaction.",
    "Product's first functional attribute.",
    "Product's second functional attribute."
    
  ),
  Category = c(
    rep("Competition-related", 8),
    rep("Platform performance", 10),
    rep("Inherent characteristic", 2)
  )
)

kable(var_descriptions, caption = "Descriptions of predictor variables used in the machine learning model.") |>
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) |>
  column_spec(1, bold = TRUE) |>
  collapse_rows(columns = 3, valign = "top")

```

# Model Development and Evaluation

## 4.1 Preprocessing Strategies

To address class imbalance and missing values in the dataset, we constructed three modeling recipes. Each recipe includes dummy encoding for categorical variables and normalization of predictors but differs in how it handles missing data and class imbalance.

-   The first recipe applies upsampling to the minority class (exited SKUs) after omitting observations with missing values. This approach balances the dataset while retaining only complete cases.
-   The second recipe uses SMOTE (Synthetic Minority Oversampling Technique) to generate synthetic examples for the minority class, again following the removal of missing values.
-   The third recipe applies SMOTE without removing missing values. This allows for the inclusion of more data points but increases the risk of introducing noise due to incomplete records.

These three recipes were designed to explore the sensitivity of model performance to different data preparation strategies.

```{r Create recipes,include=FALSE,echo = FALSE, message = FALSE}
# Recipe 1: Upsample + NA omit
ml_recipe_upsample <- recipe(exited ~ ., data = controlvariables %>% select(exited, all_of(ml_predictors))) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_naomit(everything(), skip = TRUE) %>%
  step_upsample(exited) %>%
  step_normalize(all_predictors())


# Recipe 2: SMOTE + NA omit
ml_recipe_smote <- recipe(exited ~ ., data = controlvariables %>% select(exited, all_of(ml_predictors))) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_naomit(everything(), skip = TRUE) %>%
  step_smote(exited) %>%
  step_normalize(all_predictors())



```

```{r recipe SMOTE without NA handling ,include=FALSE,echo = FALSE, message = FALSE}

# Recipe 3: SMOTE + no NA removal or imputation (risky, but as requested)
ml_recipe_smote_no_na_handling <- recipe(exited ~ ., data = controlvariables %>% select(exited, all_of(ml_predictors))) %>%
  step_dummy(all_nominal_predictors())%>%
  step_dummy(all_nominal_predictors()) 

  
```

## 4.2 Train-Test Split and Cross-Validation

To evaluate model performance, we split the dataset into training and testing sets using an 80-20 stratified split. Stratification was applied on the `exited` variable to preserve the original class proportions in both subsets. This ensures that the rare event of SKU exit remains represented in both training and testing phases.

For model tuning, we employed five-fold cross-validation on the training data, again using stratification to maintain class balance within folds. This setup improves the stability of performance estimates and helps mitigate overfitting.

```{r Split + folds setup,include=FALSE,echo = FALSE, message = FALSE }


set.seed(123)
model_data <- controlvariables %>% select(exited, all_of(ml_predictors))

data_split <- initial_split(model_data, strata = exited, prop = 0.8)
train_data <- training(data_split)
test_data  <- testing(data_split)



set.seed(123)
ml_folds <- vfold_cv(train_data, v = 5, strata = exited)


# Optional: check fold sizes
ml_folds


```

## 4.3 Classification Models

We implemented and tuned four classification algorithms to predict SKU exit:

-   A logistic regression model using the `glm` engine
-   An elastic net model using `glmnet`, with the penalty and mixing parameters tuned
-   A random forest model using the `ranger` engine with 500 trees
-   An XGBoost model using the `xgboost` engine, with tuned hyperparameters and a `scale_pos_weight` parameter set to emphasize the rare exited class

Of these, only the XGBoost model incorporated class weighting in addition to SMOTE or upsampling. This additional weighting adjustment was intended to further address the severe class imbalance in the dataset.

```{r Define models,include=FALSE,echo = FALSE, message = FALSE }

# Logistic Regression
log_reg_model <- logistic_reg(mode = "classification") %>%
  set_engine("glm")

# Elastic Net (glmnet)
elastic_net_model <- logistic_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

# Random Forest (ranger)
rf_model <- rand_forest(mode = "classification", trees = 500) %>%
  set_engine("ranger", importance = "impurity")

# XGBoost (with scale_pos_weight)
xgb_model <- boost_tree(
  trees = 500,
  learn_rate = tune(),
  tree_depth = tune(),
  loss_reduction = tune(),
  sample_size = tune(),
  mtry = tune()
) %>%
  set_engine("xgboost", scale_pos_weight = 10) %>%  # Adjust if needed
  set_mode("classification")
# 
# wf_xgb <- workflow() %>%
#   add_model(xgb_model) %>%
#   add_recipe(ml_recipe_smote)  # or whichever recipe you want


```

```{r Workflow set creation,include=FALSE,echo = FALSE, message = FALSE}
model_set1 <- workflow_set(
  preproc = list(
    upsample_recipe = ml_recipe_upsample,
    smote_recipe = ml_recipe_smote
  ),
  models = list(
    log_reg = log_reg_model,
    elastic_net = elastic_net_model,
    random_forest = rf_model,
    xgboost = xgb_model
  )
)

model_set2 <- workflow_set(
  preproc = list(
    smote_no_na_recipe = ml_recipe_smote_no_na_handling
  ),
  models = list(
    xgboost = xgb_model
  )
)

combined_model_set <- bind_rows(model_set1, model_set2)



```

## 4.4 Workflow Setup and Tuning

Each model-recipe pair was combined into a workflow using the workflowsets package and tuned using a grid search over 20 hyperparameter combinations. Tuning was conducted using five-fold cross-validation on the training set, with stratification on the exited outcome to maintain class proportions within each fold.

The following metrics were used to evaluate model performance across folds:

-   Accuracy
-   Area under the ROC curve (AUC)
-   Brier score for calibration
-   Precision
-   Recall
-   F1-score

For models like elastic net and XGBoost, tuning focused on regularization parameters (penalty, mixture) and tree-specific settings (learn_rate, tree_depth, etc.). The tune package handled the grid search and stored predictions and workflows for downstream use. Parallel processing was used to improve efficiency.

While this study used a fixed grid search, the workflow is fully compatible with more advanced methods such as Bayesian optimization (e.g., via tune_bayes() in the finetune package), which could be explored in future work.

```{r  define metrics and Run tuning ,include=FALSE,echo = FALSE, message = FALSE}
# Load required libraries


my_metrics <- metric_set(
  accuracy,
  brier_class,
  roc_auc,
  yardstick::precision,
  yardstick::recall,
  f_meas
)

# Define control
tune_control <- control_grid(
  save_pred = TRUE,
  save_workflow = TRUE,
  verbose = TRUE
)


# Run workflow_map with full metric set
tuned_models <- combined_model_set %>%
  workflow_map(
    seed = 123,
    resamples = ml_folds,
    metrics = my_metrics,
    grid = 20,
    control = tune_control
  )

```

```{r Grid tuning setup , include=FALSE,echo = FALSE, message = FALSE}



prepped_recipe <- prep(ml_recipe_smote, training = train_data)
baked_train <- bake(prepped_recipe, new_data = train_data)

baked_test  <- bake(prepped_recipe, new_data = test_data)

tuned_models <- combined_model_set %>%
  workflow_map(
    seed = 123,
    resamples = ml_folds,
    metrics = my_metrics,
    grid = 20,
    control = tune_control
  )


# Example for tuning one XGBoost workflow:
set.seed(123)


```

```{r Show notes on tuning,  echo = FALSE, include= TRUE}
show_notes(tuned_models)

```

## 4.5 Model Comparison Summary

To evaluate model performance across different preprocessing strategies and algorithms, we compared workflows built on three recipes and four classifiers. Figure \@ref(fig:model-performance-summary) summarizes the best scores for each workflow based on key metrics, including accuracy, precision, recall, F1-score, AUC, and Brier score.

-   The XGBoost model using the SMOTE recipe without missing value removal (`smote_no_na_recipe_xgboost`) consistently performed best, particularly in terms of recall and F1-score.
-   Logistic regression models performed the worst, especially under the upsample recipe, highlighting their limited capacity for capturing complex patterns in exit behavior.
-   Elastic net and random forest models performed moderately well but fell short of XGBoost on recall and overall F1-score.
-   Recall scores remained low across all workflows, emphasizing the inherent difficulty of identifying rare exit cases.

Based on these results, we selected the SMOTE-based XGBoost model with retained missing values as the final model for evaluation and interpretation.

```{r Plot best model performance,include= TRUE,echo = FALSE}
best_metrics <- tuned_models %>%
  collect_metrics() %>%
  group_by(.metric, wflow_id) %>%
  filter(mean == max(mean)) %>%
  ungroup()

ggplot(best_metrics, aes(x = mean, y = reorder(wflow_id, mean), fill = .metric)) +
  geom_col() +
  facet_wrap(~ .metric, scales = "free_x") +
  labs(
    title = "Best Scores per Workflow (3 Recipes)",
    x = "mean",
    y = NULL
  ) +
  theme_minimal()


```

## 4.6 Model Selection Rationale

While overall accuracy and AUC were relatively high across several workflows, recall was notably low—reflecting the rarity and unpredictability of exit events in the dataset. Because our research goal is to understand and predict SKU exit, recall and F1-score were treated as the most important criteria in selecting a final model.

The XGBoost model trained with the SMOTE recipe that retained missing values provided the best tradeoff between recall and overall performance. Although this approach carries the risk of introducing noise from unclean data, it was more effective in identifying exited SKUs than other configurations. For this reason, it was selected as the final model for interpretation and out-of-sample testing.

```{r Select best XGBoost model,include=FALSE,echo = FALSE}
best_xgb <- tuned_models %>%
  extract_workflow_set_result("smote_no_na_recipe_xgboost") %>%
  select_best(metric = "f_meas")


final_xgb_workflow <- finalize_workflow(
  combined_model_set %>%
    extract_workflow("smote_no_na_recipe_xgboost"),
  best_xgb
)



```

## 4.7 Final Model Training and Prediction

The selected workflow—XGBoost trained using SMOTE without removing missing values—was finalized using the best hyperparameters identified during tuning. This final model was retrained on the full training dataset and then applied to the test data.

Predicted outputs include both class labels and class probabilities, which were used to generate evaluation metrics and visualizations, including the confusion matrix and SHAP-based model interpretation.

```{r  Finalize XGBoost workflow and  Fit final model,include=FALSE,echo = FALSE}
final_xgb_workflow <- tuned_models %>%
  extract_workflow("smote_no_na_recipe_xgboost") %>%
  finalize_workflow(best_xgb)

prepped_recipe <- prep(ml_recipe_smote_no_na_handling, training = train_data)
baked_train <- bake(prepped_recipe, new_data = train_data)

final_fit <- final_xgb_workflow %>%
  remove_recipe() %>%
  add_variables(outcomes = exited, predictors = everything()) %>%
  fit(data = baked_train)


```

```{r Make test predictions,include=FALSE,echo = FALSE}
baked_test <- bake(prepped_recipe, new_data = test_data)

preds <- predict(final_fit, new_data = baked_test, type = "prob") %>%
  bind_cols(predict(final_fit, new_data = baked_test, type = "class")) %>%
  bind_cols(baked_test %>% select(exited))

```

## 4.8 Evaluation of Final Model Performance

### 4.8.1 Confusion Matrix

Figure @ref(fig-conf-matrix) displays the confusion matrix generated from the final XGBoost model’s predictions on the test set. The counts are as follows:

-   True positives (bottom-right): 28 exited SKUs correctly predicted as exited
-   True negatives (top-left): 5,911 active SKUs correctly predicted as active
-   False positives (top-right): 201 active SKUs incorrectly predicted as exited
-   False negatives (bottom-left): 1 exited SKU incorrectly predicted as active

Although the model identifies a portion of the exited SKUs, the number of false positives (201) significantly exceeds the true positives (28). This pattern indicates a tendency to over-predict exit events, which reduces the trustworthiness of its output in practice.

The model is especially prone to false alarms, flagging many SKUs as exited when they are actually active. While only one exited SKU was missed, this still matters in a rare-event prediction context. Overall, the imbalance between correctly and incorrectly predicted exits weakens the model’s practical utility.

```{r Confusion matrix,include=TRUE,echo = FALSE}
# Generate confusion matrix
conf_mat_res <- conf_mat(preds, truth = exited, estimate = .pred_class)

# Plot with smaller text for compactness
autoplot(conf_mat_res, type = "heatmap") +
  theme(text = element_text(size = 10))


```

### 4.8.2 Classification Metrics

The metrics reported in Table @ref(tab:eval-metrics-table) provide a broader view of the model’s predictive performance:

Accuracy measures overall correctness, reflecting how many SKUs were predicted correctly regardless of class.

Precision indicates how many of the SKUs predicted as exited were actually exited. Low precision here reflects the large number of false positives.

Recall captures the model’s ability to detect exited SKUs. This is particularly important in rare event detection tasks.

F1-score summarizes the balance between precision and recall. In this case, it reflects moderate performance.

ROC AUC quantifies the model’s ability to distinguish exited from active SKUs across a range of classification thresholds.

These metrics confirm that the model achieves a reasonable balance between recall and overall accuracy, but precision remains low due to frequent over-prediction of exit. This supports the view that, while the model detects some meaningful patterns, its predictions may not yet be reliable enough for automated business decisions regarding third-party seller exit.

```{r,include=TRUE,echo = FALSE}


# Make sure no conflicts from caret
#detach("package:caret", unload = TRUE)

# Define metric set
my_metrics <- yardstick::metric_set(
  yardstick::accuracy,
  yardstick::precision,
  yardstick::recall,
  yardstick::f_meas
)

# 1. Class-based metrics
class_results <- my_metrics(
  data = preds,
  truth = exited,
  estimate = .pred_class
)

# 2. Probability-based metric: ROC AUC
roc_result <- yardstick::roc_auc(
  data = preds,
  truth = exited,
  .pred_exited
)


# 3. Combine and print
all_metrics <- bind_rows(class_results, roc_result)
print(all_metrics)

```

# Global Model Explanation

To better understand which features influenced the model’s predictions, we applied three complementary global interpretation methods: (1) internal gain-based importance via the vip package, (2) permutation-based dropout loss using DALEX, and (3) partial dependence analysis for top predictors.

## 5.1. Variable Importance via XGBoost (VIP)

Figure @ref(fig-vip-xgb) ranks the top 20 predictors based on XGBoost’s internal gain metric. Notably, features like max_non_interacting_streak and click_quantile_overall appear most influential, suggesting that prolonged inactivity and lack of visibility are strong indicators of SKU exit. This aligns with the intuition that disengagement and poor platform performance precede deactivation decisions.

```{r fig-vip-xgb, include=TRUE, echo=FALSE}
vip(final_fit$fit$fit, num_features = 20, geom = "col", aesthetics = list(fill = "steelblue")) +
  ggtitle("Top 20 Predictors (XGBoost)") +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    axis.text.y = element_text(size = 10),
    axis.title = element_blank()
  )

```
## 5.2. Permutation-Based Variable Importance (Custom 0/1 Loss)

Figure @ref(fig-custom-permutation-vi) shows the variable importance results under a permutation scheme using a binary 0/1 loss function. Variables such as max_non_interacting_streak, total_sku_type_2_in_product_space, and click_quantile_overall are among the most important predictors, confirming earlier findings that disengagement and competitive intensity are key drivers of SKU exit.

Notably, the variable enter_typ_one, which indicates whether a first-party SKU entered the same product space, ranks near the bottom in terms of importance. This suggests that, in the context of this predictive model, first-party entry was not a strong direct predictor of third-party SKU deactivation. While entry may still play an indirect or structural role, it does not appear to influence exit decisions in a highly predictive way based on the observable features used in this model.
```{r fig-custom-permutation-vi, echo=FALSE, include=TRUE }
# Required libraries
library(DALEXtra)
library(ggplot2)
library(dplyr)
library(forcats)

# Define custom 0/1 loss function
binary_loss <- function(actual, predicted) {
  mean(abs(actual - predicted))
}
attr(binary_loss, "loss_name") <- "0/1 Loss"
numeric_exited <- as.numeric(baked_train$exited) - 1

xgb_explainer <- explain_tidymodels(
  final_fit,
  data = baked_train %>% select(-exited),
  y = numeric_exited,
  label = "XGBoost Exit Model"
)
# Compute permutation-based variable importance
vip_dalex <- model_parts(
  xgb_explainer,
  loss_function = binary_loss,
  N = NULL
)

# Custom plotting function
ggplot_imp <- function(...) {
  obj <- list(...)
  metric_name <- attr(obj[[1]], "loss_name")
  metric_lab <- paste(metric_name, "after permutations\n(higher = more important)")
  
  full_vip <- bind_rows(obj) %>% filter(variable != "_baseline_")
  
  perm_vals <- full_vip %>%
    filter(variable == "_full_model_") %>%
    group_by(label) %>%
    summarise(dropout_loss = mean(dropout_loss), .groups = "drop_last")
  
  p <- full_vip %>%
    filter(variable != "_full_model_") %>%
    mutate(variable = fct_reorder(variable, dropout_loss)) %>%
    ggplot(aes(x = dropout_loss, y = variable)) +
    geom_vline(data = perm_vals, aes(xintercept = dropout_loss),
               linewidth = 1.4, linetype = "dashed", alpha = 0.7) +
    geom_boxplot(fill = "#91CBD765", alpha = 0.4) +
    labs(x = metric_lab, y = NULL) +
    theme_minimal(base_size = 13) +
    theme(legend.position = "none")
  
  return(p)
}

# Generate plot
ggplot_imp(vip_dalex)

```

# Partial Dependence Profiles


Figure @ref(fig-pdp-top): Partial Dependence Profiles (PDPs) for the six most influential predictors in the XGBoost model.

This figure presents Partial Dependence Profiles (PDPs), which visualize the marginal effect of each individual feature on the predicted probability of SKU exit, holding all other variables constant. The plots reveal how changes in a single variable influence the model’s average prediction.

Key observations include:

click_quantile_overall: Exit probability decreases as visibility increases. SKUs in higher click quantiles (i.e., with more historical exposure) are less likely to exit.

max_non_interacting_streak: The probability of exit rises sharply with even short periods of inactivity. The risk plateaus after approximately 10–15 days without interaction.

type_2_activated_in_product_spacebrand_ID: A larger number of new third-party SKUs in the same brand-product space increases the likelihood of exit, likely reflecting localized competition.

total_sku_in_product_space and total_sku_type_2_in_product_space: Exit risk spikes in moderately crowded product spaces (~200–300 SKUs), suggesting crowding effects may peak at certain densities.

total_sku_type_2_in_product_spacebrand_ID: Shows a U-shaped pattern. Both low and high levels of competition within the brand-product niche appear to increase exit probability, while moderate levels pose less risk.

These profiles validate the model's emphasis on real-time engagement and local market structure as key predictors of third-party SKU survival.


```{r plot-dalex-pdp-top6, echo=FALSE, message=FALSE, warning=FALSE}
numeric_exited <- as.numeric(baked_train$exited) - 1

xgb_explainer <- explain_tidymodels(
  final_fit,
  data = baked_train %>% select(-exited),
  y = numeric_exited,
  label = "XGBoost Exit Model"
)
xgb_pdp_data <- model_profile(
  xgb_explainer,
  N = 5000,
  variables = c(
    "max_non_interacting_streak",
    "total_sku_type_2_in_product_spacebrand_ID",
    "total_sku_in_product_space",
    "NO_type_2_activated_in_product_spacebrand_ID",
    "total_sku_type_2_in_product_space",
    "click_quantile_overall"
  )
)

plot(xgb_pdp_data, geom = "aggregates", facet_ncol = 3) +
  ggtitle("Partial Dependence Profiles for Top Predictors (XGBoost)") +
  theme_minimal(base_size = 11)
```


# Conclusion

This study set out to predict third-party (3P) SKU exit on JD.com following first-party (1P) entry into product spaces, using a combination of machine learning models and interpretability tools. While the modeling pipeline was comprehensive—incorporating careful feature engineering, multiple classification algorithms, and rigorous evaluation through cross-validation—the predictive performance of all models was limited, particularly in identifying rare exit events.

Our best-performing model, an XGBoost classifier trained with SMOTE and class weighting, achieved moderate recall and F1-score but suffered from high false positive rates. These outcomes suggest that third-party SKU exit is not only rare but also difficult to anticipate based on observable features alone. This unpredictability may reflect the influence of unobserved strategic decisions, seller-specific constraints, or platform-level interventions not captured in the dataset.

As a result, the model’s explanatory tools—such as feature importance rankings and partial dependence plots—should be interpreted with caution. While certain patterns (e.g., prolonged inactivity, high crowding) appear consistently influential across multiple explanation techniques, their practical relevance is limited by the model’s low precision and weak out-of-sample performance. In essence, when a model cannot reliably predict an outcome, its explanations offer limited actionable value.

Future research could benefit from richer longitudinal data, seller-level covariates, and more precise identification of platform entry events. Additionally, causal inference methods may complement predictive models by isolating structural effects of platform competition on seller survival. Until then, predictive modeling of 3P exit on hybrid platforms remains a challenging task—highlighting the complexity of strategic behavior in competitive online marketplaces.

#Refrence
