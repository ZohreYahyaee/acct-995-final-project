---
title: "Spillover Effects of Platform Entry: Predicting Third-Party Seller Exit on JD.com"
author: "Zohre Yahyaee"
date: today
format: pdf
bibliography: references.bib
csl: apa.csl
abstract: |
  This study investigates whether first-party (1P) entry into product spaces on JD.com increases the likelihood of third-party (3P) SKU exit. Using SKU-level panel data and a range of classification models, we find that exit appears to occur largely at random, which limits the predictive power of all tested models. Nevertheless, we observe that features capturing lack of recent orders and user interaction provide some explanatory value. Our best-performing model, an XGBoost classifier tuned via Bayesian optimization and interpreted using SHAP values, identifies disengagement and product space crowding as important—but ultimately insufficient—signals for accurately predicting exit behavior.
---

```{r theme, include=FALSE,echo = FALSE, message = FALSE}
theme_set(theme_minimal(base_size = 12))
theme_update(
  plot.title = element_text(hjust = 0.5, face = "bold"),
  axis.title = element_text(face = "bold")
)

```

# Introduction

## 1.1 What is the research question?

In this research, we investigate the dynamics of hybrid e-commerce platforms, where the platform plays a dual role: acting both as a retailer by selling its own products and as a platform operator facilitating third-party sellers. This dual role can create a conflict of interest, as the platform both supports and competes with third-party sellers. Prior research suggests that the entry of the platform as a first-party seller can negatively impact third-party sellers, potentially leading some of them to exit the market @zhu2018competing; @jiang2011firm. Our goal is to empirically examine this phenomenon and understand how the platform’s dual role influences third-party seller behavior.

## 1.2. Why is this research question important?

This question is important because the hybrid platform business model has become ubiquitous among online platforms globally. For example, Amazon in the U.S. has utilized this model since the early 2000s, while Alibaba and JD.com in China also follow similar practices @hu2024supercharged; @wells2018amazon. Understanding the impact of this model is crucial for both policymakers and businesses, as it shapes market competition and affects seller sustainability. While hybrid platforms offer benefits through increased consumer reach and operational efficiency, they also raise concerns about fair competition and platform bias—particularly when the platform uses its data or visibility advantages to undermine independent sellers @crawford2022amazon; @he2020impact.

[^1]

[^1]: I thank Professor Eric Weisbrod for guidance and patience for this challenging project. This project was completed for ACCT 995 at the University of Kansas.

## 1.3. What is the variable of interest?

The main variable of interest is a binary classification outcome indicating whether a third-party SKU deactivated (i.e., exited) within the observed time period. This allows us to analyze the likelihood and predictors of seller exit from the platform.


# Literature Review

The rise of hybrid platforms—where the platform competes directly with the sellers it hosts—has raised concerns about fair competition and self-preferencing behavior. Foundational theoretical and empirical work by @jiang2011firm and @zhu2018competing suggests that platform entry is often strategic, targeting successful product spaces based on third-party seller performance. While this allows platforms to reduce uncertainty, it also creates potential harm: platform entry is associated with reduced sales, lower visibility, and even exit for affected third-party sellers. These findings support the view that platform entry can crowd out independent merchants, though the magnitude and mechanisms of these effects may vary across product types.

On one hand, @zhu2018competing show that Amazon's entry reduces third-party sales and discourages continued participation. Similarly, @song2020spillover document that sellers anticipate platform entry and adjust their product variety accordingly. These studies highlight the competitive threat that platform entry can pose.

On the other hand, @deng2023can and @chi2022competition present evidence for positive spillovers: when the platform enters a product space, it may boost overall demand through marketing and consumer awareness, indirectly benefiting existing third-party sellers. In some cases, third-party sales even rise after platform entry.

The divergence in these findings points to the role of context—such as product popularity, brand strength, and timing of entry. Some products may benefit from visibility spillovers, while others suffer from direct substitution. However, few studies systematically examine whether such spillover effects translate into exit behavior. In this project, we shift the focus from pricing and sales to the more fundamental question of market survival. By predicting third-party exit, we offer new insight into how platform dynamics shape seller persistence under competitive pressure.

```{r setup, include=FALSE,echo = FALSE, message = FALSE}
#library(tidymodels)
library(themis)
library(dplyr)
library(ggplot2)
library(lubridate)
library(vip)
library(DALEXtra)
library(workflowsets)
library(doParallel)
library(glue)
library(patchwork)  # for combining plots
library(forcats)
library(dplyr)
library(ggplot2)
library(yardstick)
library(tune)
library(workflows)
library(rsample )
# Define your metrics
library(yardstick)
library(doFuture)

library(tune)
library(finetune)
library(parsnip)
library(workflows)
library(yardstick)
library(dplyr)
library(dials)
library(yardstick)
library(dplyr)
library(vip)
registerDoFuture()
plan(multisession, workers = parallel::detectCores() - 1)

```

```{r Load data, include=FALSE,echo = FALSE, message = FALSE}

source("00_setup.R")

df <-read_csv(glue("{data_path}/balanced_panel_with_clicks_and_order_sku.csv"))

# Preview
head(df)
summary(df)

```

#Dataset Overview
## 3.1. What is the dataset about?

This study draws on SKU-level transaction data from JD.com, a major hybrid e-commerce platform in China. The dataset spans a single month—March 2018—and provides detailed information about each SKU’s commercial activity and platform presence. For each SKU, the data includes:

-   Product characteristics (e.g., brand, functional attributes),
-   Activation and deactivation dates (capturing SKU entry and exit),
-   User interaction measures (e.g., daily clicks, views), and
-   Purchase activity (e.g., daily order counts).

An important feature of this dataset is that JD.com maintains separate listings for each seller-SKU pair, allowing for precise tracking of SKU engagement over time. This structure provides a clean empirical setting to analyze third-party (3P) seller behavior—particularly decisions to deactivate a SKU—amid potential competition from platform-backed first-party (1P) sellers @shen2024jd.

## 3.2. What are the dataset challenges?

This dataset offers rich transactional data, but several limitations complicate the analysis:

-   **Short observation window**: The data covers a single month (March 2018), which restricts our ability to observe long-term exit patterns or delayed responses to platform entry.
-   **Incomplete attribute data**: Many SKUs lack complete information on core product characteristics, making it difficult to consistently group them into functionally comparable product spaces. As a result, we focus our analysis on the subset of SKUs with non-missing values for both functional attributes and brand identifiers.
-   **Defining true entry events**: JD.com’s structure makes it challenging to isolate clean instances of first-party entry. We mitigate this by identifying product spaces that transitioned from exclusive third-party to joint presence with first-party SKUs during the sample period @shen2024jd.

## 3.3. Exploratory Data Analysis (EDA)

We begin with a descriptive overview of the dataset:

-   The dataset includes 31,876 unique SKUs, each linked to a seller type, brand, and product attributes.
-   Over 20 million user click events and approximately 500,000 purchase transactions are recorded, capturing detailed consumer interactions @shen2024jd.
-   Despite the scale, only 9,159 SKUs received at least one order during the observation window, highlighting the skewed nature of demand @ernst2006estimating.
-   Preliminary comparisons reveal sharp contrasts between seller types:\
    First-party SKUs received an average of 223 orders, while third-party SKUs averaged only 9 orders.

These disparities underscore the need to examine whether platform entry is associated with reduced visibility or demand for third-party sellers—raising the question of whether first-party participation displaces existing market participants @zhu2018competing; @deng2023can.

```{r Filter to 3P SKUs and define target variable, include=FALSE,echo = FALSE, message = FALSE}

# Step 1: Filter to 3P SKUs and define target variable
sku_data <- df %>%
  filter(type == 2) %>%
  mutate(
    exited = factor(!is.na(deactivate_date), levels = c(FALSE, TRUE), labels = c("active", "exited"))
  )

# Step 2: Make sure categorical variables are explicit
sku_data <- sku_data %>%
  mutate(
    attribute1 = forcats::fct_explicit_na(as.factor(attribute1), na_level = "Missing"),
    attribute2 = forcats::fct_explicit_na(as.factor(attribute2), na_level = "Missing")
  )

# Quick check
table(sku_data$exited)
```

```{r Interaction streak and price quantiles, include=FALSE,echo = FALSE, message = FALSE }


# Step 1: Create date-based indicators
sku_data <- sku_data %>%
  mutate(
    activate_day = day(as.POSIXct(activate_date, origin = "1970-01-01")),
    deactivate_day = day(as.POSIXct(deactivate_date, origin = "1970-01-01")),
    is_interacting = NO_sales_within_day > 0 | number_of_unique_users_clicked_daily > 0
  )

# Step 2: Create interaction streak
sku_data <- sku_data %>%
  mutate(in_active_window = is.na(deactivate_day) | day_of_month <= deactivate_day) %>%
  arrange(sku_ID, day_of_month) %>%
  group_by(sku_ID) %>%
  mutate(
    non_interacting_streak = {
      streak <- numeric(n())
      for (i in seq_along(is_interacting)) {
        if (i == 1) {
          streak[i] <- ifelse(!is_interacting[i] & in_active_window[i], 1, 0)
        } else {
          streak[i] <- ifelse(!is_interacting[i] & in_active_window[i], streak[i-1] + 1, 0)
        }
      }
      streak
    }
  ) %>%
  ungroup() %>%
  group_by(sku_ID) %>%
  mutate(max_non_interacting_streak = max(non_interacting_streak)) %>%
  ungroup() %>%
  select(-in_active_window)

# Step 3: Price quantiles
priced_skus <- sku_data %>%
  filter(!is.na(imputed_ave_final_unit_price)) %>%
  group_by(product_space) %>%
  mutate(price_quantile_product = ntile(imputed_ave_final_unit_price, 10)) %>%
  ungroup() %>%
  group_by(brand_ID) %>%
  mutate(price_quantile_brand = ntile(imputed_ave_final_unit_price, 10)) %>%
  ungroup() %>%
  mutate(price_quantile_overall = ntile(imputed_ave_final_unit_price, 10))

# Step 4: Merge quantiles back
price_quantiles <- priced_skus %>%
  group_by(sku_ID) %>%
  summarise(
    price_quantile_product = first(price_quantile_product),
    price_quantile_brand = first(price_quantile_brand),
    price_quantile_overall = first(price_quantile_overall)
  )

sku_data <- sku_data %>%
  left_join(price_quantiles, by = "sku_ID")
```

```{r order-click-quantiles, include=FALSE,echo = FALSE, message = FALSE}
# Step 1: Summarize orders and clicks at the product space level
agg_stats <- sku_data %>%
  group_by(product_space) %>%
  summarise(
    total_orders_product_space = sum(NO_sales_within_day, na.rm = TRUE),
    total_clicks_product_space = sum(click_including_refresh_daily, na.rm = TRUE)
  ) %>%
  ungroup()

# Step 2: Join these back to SKU-level data
sku_data <- sku_data %>%
  left_join(agg_stats, by = "product_space")

# Step 3: Compute quantiles for orders
order_quantiles <- sku_data %>%
  filter(!is.na(total_orders_product_space)) %>%
  group_by(product_space) %>%
  mutate(order_quantile_product = ntile(total_orders_product_space, 10)) %>%
  ungroup() %>%
  group_by(brand_ID) %>%
  mutate(order_quantile_brand = ntile(total_orders_product_space, 10)) %>%
  ungroup() %>%
  mutate(order_quantile_overall = ntile(total_orders_product_space, 10))

# Step 4: Compute quantiles for clicks
click_quantiles <- order_quantiles %>%
  filter(!is.na(total_clicks_product_space)) %>%
  group_by(product_space) %>%
  mutate(click_quantile_product = ntile(total_clicks_product_space, 10)) %>%
  ungroup() %>%
  group_by(brand_ID) %>%
  mutate(click_quantile_brand = ntile(total_clicks_product_space, 10)) %>%
  ungroup() %>%
  mutate(click_quantile_overall = ntile(total_clicks_product_space, 10))

# Step 5: Reduce to one row per SKU and merge back
click_order_summary <- click_quantiles %>%
  group_by(sku_ID) %>%
  summarise(
    order_quantile_product = first(order_quantile_product),
    order_quantile_brand = first(order_quantile_brand),
    order_quantile_overall = first(order_quantile_overall),
    click_quantile_product = first(click_quantile_product),
    click_quantile_brand = first(click_quantile_brand),
    click_quantile_overall = first(click_quantile_overall)
  )

sku_data <- sku_data %>%
  left_join(click_order_summary, by = "sku_ID")
```

```{r Aggregate to SKU level,echo = FALSE, message = FALSE}

# Define variables to keep
static_vars <- c(
  "attribute1", "attribute2", "brand_ID", "product_space",
  "enter_typ_one", "NO_type_2_activated_in_product_space",
  "total_sku_type_1_in_product_space", "total_sku_type_2_in_product_space", "total_sku_in_product_space",
  "enter_typ_one_within_brand", "NO_type_1_activated_in_product_spacebrand_ID",
  "NO_type_2_activated_in_product_spacebrand_ID", "total_sku_type_1_in_product_spacebrand_ID",
  "total_sku_type_2_in_product_spacebrand_ID", "total_sku_in_product_spacebrand_ID",
  "max_non_interacting_streak", "exited",
  "price_quantile_product", "price_quantile_brand", "price_quantile_overall",
  "order_quantile_product", "order_quantile_brand", "order_quantile_overall",
  "click_quantile_product", "click_quantile_brand", "click_quantile_overall"
)

# Aggregate to SKU level
controlvariables <- sku_data %>%
  group_by(sku_ID) %>%
  summarise(across(all_of(static_vars), ~ first(.x))) %>%
  ungroup()



# Quick structure check
#glimpse(controlvariables)
table(controlvariables$exited)

```

```{r Define predictor variables,include=FALSE,echo = FALSE, message = FALSE}

ml_predictors <- c(
  "enter_typ_one",
  "total_sku_type_1_in_product_space",
  "total_sku_type_2_in_product_space",
  "total_sku_in_product_space",
  "price_quantile_overall",
  "price_quantile_product",
  "price_quantile_brand",
  "order_quantile_product",
  "order_quantile_brand",
  "order_quantile_overall",
  "click_quantile_product",
  "click_quantile_brand",
  "click_quantile_overall",
  "attribute1", 
  "attribute2",
  "max_non_interacting_streak",
  "total_sku_type_1_in_product_spacebrand_ID",
  "total_sku_type_2_in_product_spacebrand_ID",
  "NO_type_2_activated_in_product_spacebrand_ID",
  "NO_type_1_activated_in_product_spacebrand_ID"
)

```

## 3.4 Feature Engineering

To support predictive modeling, we engineered a set of variables that reflect three core aspects of SKU behavior: the level of competition in the product space, product characteristics, and indicators of platform performance. The following paragraphs describe the purpose and construction of each group of features.

- The feature `max_non_interacting_streak` was created to capture disengagement. This variable records the maximum number of consecutive days during which a SKU received neither a click nor an order. It serves as a proxy for inactivity and is intended to reflect declining customer interest or visibility over time.

- To control for variation in scale across categories, we constructed quantile-based rankings for prices, orders, and clicks. These were computed separately at three levels: within product space, within brand, and across the entire catalog. The resulting variables include price quantile within product space (`price_quantile_product`), click quantile within brand (`click_quantile_brand`), and others. These features help capture a SKU's relative position in terms of price and demand.

- Several features were included to measure competition within a SKU’s environment. These include the total number of SKUs in the product space, the number of first-party and third-party SKUs within the same brand and product group, and whether a first-party SKU entered the space. Together, these variables are intended to capture the intensity of local competition and potential crowding effects.

- Product-level attributes were retained as categorical variables. The fields `attribute1` and `attribute2` represent the functional classification of each SKU. To avoid dropping observations with incomplete data, missing values were explicitly labeled as "Missing" and treated as a separate category.

Table \@ref(tab:variable-description-table) presents the key variables used in the prediction model. These variables span three conceptual groups: (1) competitive pressure, such as the number of rival SKUs in a product space; (2) inherent product or brand characteristics, including categorical attributes and brand identity; and (3) platform performance indicators, such as click volume, order quantiles, and recent interaction history.


```{r var-description-table, message=FALSE, warning=FALSE}
library(knitr)
library(tibble)
library(kableExtra)

var_descriptions <- tibble::tibble(
  Variable = c(
    "enter_typ_one",
    "total_sku_type_1_in_product_space",
    "total_sku_type_2_in_product_space",
    "total_sku_in_product_space",
    "total_sku_type_1_in_product_spacebrand_ID",
    "total_sku_type_2_in_product_spacebrand_ID",
    "NO_type_2_activated_in_product_spacebrand_ID",
    "NO_type_1_activated_in_product_spacebrand_ID",
    "price_quantile_overall",
    "price_quantile_product",
    "price_quantile_brand",
    "order_quantile_product",
    "order_quantile_brand",
    "order_quantile_overall",
    "click_quantile_product",
    "click_quantile_brand",
    "click_quantile_overall",
    "max_non_interacting_streak",
    "attribute1",
    "attribute2"
    
  ),
  Description = c(
    "Indicates whether a 1P SKU entered the product space.",
    "Number of 1P SKUs in the product space.",
    "Number of 3P SKUs in the product space.",
    "Total number of SKUs in the product space.",
    "Number of 1P SKUs in product space within brand.",
    "Number of 3P SKUs in product space within brand.",
    "Number of newly activated 3P SKUs in brand-product space.",
    "Number of newly activated 1P SKUs in brand-product space.",
    "SKU's price quantile across all SKUs.",
    "SKU's price quantile within product space.",
    "SKU's price quantile within brand.",
    "SKU's order quantile within product space.",
    "SKU's order quantile within brand.",
    "SKU's order quantile overall.",
    "SKU's click quantile within product space.",
    "SKU's click quantile within brand.",
    "SKU's click quantile overall.",
    "Max consecutive days with no interaction.",
    "Product's first functional attribute.",
    "Product's second functional attribute."
    
  ),
  Category = c(
    rep("Competition-related", 8),
    rep("Platform performance", 10),
    rep("Inherent characteristic", 2)
  )
)

kable(var_descriptions, caption = "Descriptions of predictor variables used in the machine learning model.") |>
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) |>
  column_spec(1, bold = TRUE) |>
  collapse_rows(columns = 3, valign = "top")

```

# Model Development and Evaluation

## 4.1 Preprocessing Strategies

To address class imbalance and missing values in the dataset, we constructed three modeling recipes. Each recipe includes dummy encoding for categorical variables and normalization of predictors but differs in how it handles missing data and class imbalance.

- The first recipe applies upsampling to the minority class (exited SKUs) after omitting observations with missing values. This approach balances the dataset while retaining only complete cases.
- The second recipe uses SMOTE (Synthetic Minority Oversampling Technique) to generate synthetic examples for the minority class, again following the removal of missing values.
- The third recipe applies SMOTE without removing missing values. This allows for the inclusion of more data points but increases the risk of introducing noise due to incomplete records.

These three recipes were designed to explore the sensitivity of model performance to different data preparation strategies.


```{r Create recipes,include=FALSE,echo = FALSE, message = FALSE}
# Recipe 1: Upsample + NA omit
ml_recipe_upsample <- recipe(exited ~ ., data = controlvariables %>% select(exited, all_of(ml_predictors))) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_naomit(everything(), skip = TRUE) %>%
  step_upsample(exited) %>%
  step_normalize(all_predictors())


# Recipe 2: SMOTE + NA omit
ml_recipe_smote <- recipe(exited ~ ., data = controlvariables %>% select(exited, all_of(ml_predictors))) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_naomit(everything(), skip = TRUE) %>%
  step_smote(exited) %>%
  step_normalize(all_predictors())



```

```{r recipe SMOTE without NA handling ,include=FALSE,echo = FALSE, message = FALSE}

# Recipe 3: SMOTE + no NA removal or imputation (risky, but as requested)
ml_recipe_smote_no_na_handling <- recipe(exited ~ ., data = controlvariables %>% select(exited, all_of(ml_predictors))) %>%
  step_dummy(all_nominal_predictors())%>%
  step_dummy(all_nominal_predictors()) 

  
```


## 4.2 Train-Test Split and Cross-Validation

To evaluate model performance, we split the dataset into training and testing sets using an 80-20 stratified split. Stratification was applied on the `exited` variable to preserve the original class proportions in both subsets. This ensures that the rare event of SKU exit remains represented in both training and testing phases.

For model tuning, we employed five-fold cross-validation on the training data, again using stratification to maintain class balance within folds. This setup improves the stability of performance estimates and helps mitigate overfitting.


```{r Split + folds setup,include=FALSE,echo = FALSE, message = FALSE }


set.seed(123)
model_data <- controlvariables %>% select(exited, all_of(ml_predictors))

data_split <- initial_split(model_data, strata = exited, prop = 0.8)
train_data <- training(data_split)
test_data  <- testing(data_split)



set.seed(123)
ml_folds <- vfold_cv(train_data, v = 5, strata = exited)


# Optional: check fold sizes
ml_folds


```

## 4.3 Classification Models

We implemented and tuned four classification algorithms to predict SKU exit:

- A logistic regression model using the `glm` engine
- An elastic net model using `glmnet`, with the penalty and mixing parameters tuned
- A random forest model using the `ranger` engine with 500 trees
- An XGBoost model using the `xgboost` engine, with tuned hyperparameters and a `scale_pos_weight` parameter set to emphasize the rare exited class

Of these, only the XGBoost model incorporated class weighting in addition to SMOTE or upsampling. This additional weighting adjustment was intended to further address the severe class imbalance in the dataset.


```{r Define models,include=FALSE,echo = FALSE, message = FALSE }

# Logistic Regression
log_reg_model <- logistic_reg(mode = "classification") %>%
  set_engine("glm")

# Elastic Net (glmnet)
elastic_net_model <- logistic_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

# Random Forest (ranger)
rf_model <- rand_forest(mode = "classification", trees = 500) %>%
  set_engine("ranger", importance = "impurity")

# XGBoost (with scale_pos_weight)
xgb_model <- boost_tree(
  trees = 500,
  learn_rate = tune(),
  tree_depth = tune(),
  loss_reduction = tune(),
  sample_size = tune(),
  mtry = tune()
) %>%
  set_engine("xgboost", scale_pos_weight = 10) %>%  # Adjust if needed
  set_mode("classification")
# 
# wf_xgb <- workflow() %>%
#   add_model(xgb_model) %>%
#   add_recipe(ml_recipe_smote)  # or whichever recipe you want


```

```{r Workflow set creation,include=FALSE,echo = FALSE, message = FALSE}
model_set1 <- workflow_set(
  preproc = list(
    upsample_recipe = ml_recipe_upsample,
    smote_recipe = ml_recipe_smote
  ),
  models = list(
    log_reg = log_reg_model,
    elastic_net = elastic_net_model,
    random_forest = rf_model,
    xgboost = xgb_model
  )
)

model_set2 <- workflow_set(
  preproc = list(
    smote_no_na_recipe = ml_recipe_smote_no_na_handling
  ),
  models = list(
    xgboost = xgb_model
  )
)

combined_model_set <- bind_rows(model_set1, model_set2)



```

## 4.4 Workflow Setup and Tuning

Each model-recipe pair was combined into a workflow using the workflowsets package and tuned using a grid search over 20 hyperparameter combinations. Tuning was conducted using five-fold cross-validation on the training set, with stratification on the exited outcome to maintain class proportions within each fold.

The following metrics were used to evaluate model performance across folds:

- Accuracy
- Area under the ROC curve (AUC)
- Brier score for calibration
- Precision
- Recall
- F1-score

For models like elastic net and XGBoost, tuning focused on regularization parameters (penalty, mixture) and tree-specific settings (learn_rate, tree_depth, etc.). The tune package handled the grid search and stored predictions and workflows for downstream use. Parallel processing was used to improve efficiency.

While this study used a fixed grid search, the workflow is fully compatible with more advanced methods such as Bayesian optimization (e.g., via tune_bayes() in the finetune package), which could be explored in future work.
```{r  define metrics and Run tuning ,include=FALSE,echo = FALSE, message = FALSE}
# Load required libraries


my_metrics <- metric_set(
  accuracy,
  brier_class,
  roc_auc,
  yardstick::precision,
  yardstick::recall,
  f_meas
)

# Define control
tune_control <- control_grid(
  save_pred = TRUE,
  save_workflow = TRUE,
  verbose = TRUE
)


# Run workflow_map with full metric set
tuned_models <- combined_model_set %>%
  workflow_map(
    seed = 123,
    resamples = ml_folds,
    metrics = my_metrics,
    grid = 20,
    control = tune_control
  )

```

```{r Grid tuning setup , include=FALSE,echo = FALSE, message = FALSE}



prepped_recipe <- prep(ml_recipe_smote, training = train_data)
baked_train <- bake(prepped_recipe, new_data = train_data)

baked_test  <- bake(prepped_recipe, new_data = test_data)

tuned_models <- combined_model_set %>%
  workflow_map(
    seed = 123,
    resamples = ml_folds,
    metrics = my_metrics,
    grid = 20,
    control = tune_control
  )


# Example for tuning one XGBoost workflow:
set.seed(123)


```

```{r Show notes on tuning,  echo = FALSE, include= TRUE}
show_notes(tuned_models)

```
## 4.5 Model Comparison Summary

To evaluate model performance across different preprocessing strategies and algorithms, we compared workflows built on three recipes and four classifiers. Figure \@ref(fig:model-performance-summary) summarizes the best scores for each workflow based on key metrics, including accuracy, precision, recall, F1-score, AUC, and Brier score.

- The XGBoost model using the SMOTE recipe without missing value removal (`smote_no_na_recipe_xgboost`) consistently performed best, particularly in terms of recall and F1-score.
- Logistic regression models performed the worst, especially under the upsample recipe, highlighting their limited capacity for capturing complex patterns in exit behavior.
- Elastic net and random forest models performed moderately well but fell short of XGBoost on recall and overall F1-score.
- Recall scores remained low across all workflows, emphasizing the inherent difficulty of identifying rare exit cases.

Based on these results, we selected the SMOTE-based XGBoost model with retained missing values as the final model for evaluation and interpretation.


```{r Plot best model performance,include= TRUE,echo = FALSE}
best_metrics <- tuned_models %>%
  collect_metrics() %>%
  group_by(.metric, wflow_id) %>%
  filter(mean == max(mean)) %>%
  ungroup()

ggplot(best_metrics, aes(x = mean, y = reorder(wflow_id, mean), fill = .metric)) +
  geom_col() +
  facet_wrap(~ .metric, scales = "free_x") +
  labs(
    title = "Best Scores per Workflow (3 Recipes)",
    x = "mean",
    y = NULL
  ) +
  theme_minimal()


```
## 4.6 Model Selection Rationale

While overall accuracy and AUC were relatively high across several workflows, recall was notably low—reflecting the rarity and unpredictability of exit events in the dataset. Because our research goal is to understand and predict SKU exit, recall and F1-score were treated as the most important criteria in selecting a final model.

The XGBoost model trained with the SMOTE recipe that retained missing values provided the best tradeoff between recall and overall performance. Although this approach carries the risk of introducing noise from unclean data, it was more effective in identifying exited SKUs than other configurations. For this reason, it was selected as the final model for interpretation and out-of-sample testing.

```{r Select best XGBoost model,include=FALSE,echo = FALSE}
best_xgb <- tuned_models %>%
  extract_workflow_set_result("smote_no_na_recipe_xgboost") %>%
  select_best(metric = "f_meas")


final_xgb_workflow <- finalize_workflow(
  combined_model_set %>%
    extract_workflow("smote_no_na_recipe_xgboost"),
  best_xgb
)



```

## 4.7 Final Model Training and Prediction

The selected workflow—XGBoost trained using SMOTE without removing missing values—was finalized using the best hyperparameters identified during tuning. This final model was retrained on the full training dataset and then applied to the test data.

Predicted outputs include both class labels and class probabilities, which were used to generate evaluation metrics and visualizations, including the confusion matrix and SHAP-based model interpretation.

```{r  Finalize XGBoost workflow and  Fit final model,include=FALSE,echo = FALSE}
final_xgb_workflow <- tuned_models %>%
  extract_workflow("smote_no_na_recipe_xgboost") %>%
  finalize_workflow(best_xgb)

prepped_recipe <- prep(ml_recipe_smote_no_na_handling, training = train_data)
baked_train <- bake(prepped_recipe, new_data = train_data)

final_fit <- final_xgb_workflow %>%
  remove_recipe() %>%
  add_variables(outcomes = exited, predictors = everything()) %>%
  fit(data = baked_train)


```

```{r Make test predictions,include=FALSE,echo = FALSE}
baked_test <- bake(prepped_recipe, new_data = test_data)

preds <- predict(final_fit, new_data = baked_test, type = "prob") %>%
  bind_cols(predict(final_fit, new_data = baked_test, type = "class")) %>%
  bind_cols(baked_test %>% select(exited))

```

## 4.8 Evaluation of Final Model Performance
### 4.8.1 Confusion Matrix
Figure @ref(fig-conf-matrix) displays the confusion matrix generated from the final XGBoost model’s predictions on the test set. The counts are as follows:

- True positives (bottom-right): 28 exited SKUs correctly predicted as exited
- True negatives (top-left): 5,911 active SKUs correctly predicted as active
- False positives (top-right): 201 active SKUs incorrectly predicted as exited
- False negatives (bottom-left): 1 exited SKU incorrectly predicted as active

Although the model identifies a portion of the exited SKUs, the number of false positives (201) significantly exceeds the true positives (28). This pattern indicates a tendency to over-predict exit events, which reduces the trustworthiness of its output in practice.

The model is especially prone to false alarms, flagging many SKUs as exited when they are actually active. While only one exited SKU was missed, this still matters in a rare-event prediction context. Overall, the imbalance between correctly and incorrectly predicted exits weakens the model’s practical utility.

```{r Confusion matrix,include=TRUE,echo = FALSE}
# Generate confusion matrix
conf_mat_res <- conf_mat(preds, truth = exited, estimate = .pred_class)

# Plot with smaller text for compactness
autoplot(conf_mat_res, type = "heatmap") +
  theme(text = element_text(size = 10))


```

### 4.8.2 Classification Metrics

The metrics reported in Table @ref(tab:eval-metrics-table) provide a broader view of the model’s predictive performance:

Accuracy measures overall correctness, reflecting how many SKUs were predicted correctly regardless of class.

Precision indicates how many of the SKUs predicted as exited were actually exited. Low precision here reflects the large number of false positives.

Recall captures the model’s ability to detect exited SKUs. This is particularly important in rare event detection tasks.

F1-score summarizes the balance between precision and recall. In this case, it reflects moderate performance.

ROC AUC quantifies the model’s ability to distinguish exited from active SKUs across a range of classification thresholds.

These metrics confirm that the model achieves a reasonable balance between recall and overall accuracy, but precision remains low due to frequent over-prediction of exit. This supports the view that, while the model detects some meaningful patterns, its predictions may not yet be reliable enough for automated business decisions regarding third-party seller exit.

```{r,include=TRUE,echo = FALSE}


# Make sure no conflicts from caret
#detach("package:caret", unload = TRUE)

# Define metric set
my_metrics <- yardstick::metric_set(
  yardstick::accuracy,
  yardstick::precision,
  yardstick::recall,
  yardstick::f_meas
)

# 1. Class-based metrics
class_results <- my_metrics(
  data = preds,
  truth = exited,
  estimate = .pred_class
)

# 2. Probability-based metric: ROC AUC
roc_result <- yardstick::roc_auc(
  data = preds,
  truth = exited,
  .pred_exited
)


# 3. Combine and print
all_metrics <- bind_rows(class_results, roc_result)
print(all_metrics)

```

##3. Global Model Explanation

We use three complementary approaches to interpret the global feature importance of our XGBoost model: (1) built-in variable importance via the `vip` package, (2) dropout loss-based importance from DALEX, and (3) permutation-based variable importance using custom loss functions. Together, these methods reveal which features contribute most significantly to exit predictions.

### 3.1. Variable Importance via XGBoost (VIP)

```{r VIP, include=TRUE, echo=FALSE}
vip(final_fit$fit$fit, num_features = 20, geom = "col", aesthetics = list(fill = "steelblue")) +
  ggtitle("Top 20 Predictors (XGBoost)") +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    axis.text.y = element_text(size = 10),
    axis.title = element_blank()
  )

```

This plot uses the internal gain metric from XGBoost to rank variables. Features such as `max_non_interacting_streak` and `click_quantile_overall` emerge as dominant drivers.

------------------------------------------------------------------------

### 3.2. Dropout Loss Importance via DALEX

```{r DALEX explainer + global VI, message=FALSE, warning=FALSE, include=TRUE, echo=FALSE}

numeric_exited <- as.numeric(baked_train$exited) - 1
suppressMessages(
  suppressWarnings(
    {
      tmp <- capture.output({
        xgb_explainer <- explain_tidymodels(
          model = final_fit,
          data = baked_train %>% select(-exited),
          y = numeric_exited,
          label = "XGBoost Exit Model"
        )
      })
    }
  )
)


library(ggplot2)
library(dplyr)
library(forcats)

vi_box <- vi_dalex %>%
  filter(variable != "_baseline_", variable != "_full_model_") %>%
  mutate(variable = fct_reorder(variable, dropout_loss))

baseline <- vi_dalex %>%
  filter(variable == "_full_model_") %>%
  summarise(dropout_loss = mean(dropout_loss))

ggplot(vi_box, aes(x = dropout_loss, y = variable)) +
  geom_vline(xintercept = baseline$dropout_loss, linetype = "dashed", color = "gray40", linewidth = 1) +
  geom_boxplot(fill = "#0072B2", alpha = 0.5, outlier.size = 0.7) +
  labs(
    title = "Variable Importance (Dropout Loss with Uncertainty)",
    subtitle = "DALEX model_parts output with boxplots",
    x = "Dropout Loss",
    y = NULL
  ) +
  theme_minimal(base_size = 13) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5),
    plot.subtitle = element_text(hjust = 0.5),
    axis.text.y = element_text(size = 9),
    panel.grid.major.y = element_blank()
  )


```

DALEX assesses variable importance by measuring prediction loss when variables are permuted. The output confirms the prominence of behavioral and competition-related variables.

------------------------------------------------------------------------

### 3.3. Permutation-Based Variable Importance with Custom Loss

```{r DALEX permutation VI, include=TRUE, echo=FALSE}
binary_loss <- function(actual, predicted) {
  mean(abs(actual - predicted))
}
attr(binary_loss, "loss_name") <- "0/1 Loss"

vip_dalex <- model_parts(xgb_explainer, loss_function = binary_loss, N = NULL)

ggplot_imp <- function(...) {
  obj <- list(...)
  metric_name <- attr(obj[[1]], "loss_name")
  metric_lab <- paste(metric_name, "after permutations\n(higher = more important)")
  full_vip <- bind_rows(obj) %>% filter(variable != "_baseline_")
  perm_vals <- full_vip %>% filter(variable == "_full_model_") %>%
    group_by(label) %>% summarise(dropout_loss = mean(dropout_loss))
  p <- full_vip %>%
    filter(variable != "_full_model_") %>%
    mutate(variable = fct_reorder(variable, dropout_loss)) %>%
    ggplot(aes(dropout_loss, variable)) +
    geom_vline(data = perm_vals, aes(xintercept = dropout_loss), linewidth = 1.4, lty = 2, alpha = 0.7) +
    geom_boxplot(fill = "#91CBD765", alpha = 0.4) +
    theme_minimal(base_size = 13) +
    theme(legend.position = "none") +
    labs(x = metric_lab, y = NULL)
  p
}

ggplot_imp(vip_dalex)


library(patchwork)
p1 <- plot(xgb_pdp_data, variables = "max_non_interacting_streak", geom = "lines")
p2 <- plot(xgb_pdp_data, variables = "total_sku_type_2_in_product_spacebrand_ID", geom = "lines")

(p1 | p2) + plot_annotation(title = "Partial Dependence Plots")

```

This method provides uncertainty estimates and confirms the robustness of key predictors. Boxplot widths indicate variation across permutations.

------------------------------------------------------------------------

### 3.4. Partial Dependence Analysis

```{r setup-dalex-explainer, echo=FALSE, include=FALSE, message=FALSE, warning=FALSE}
library(DALEXtra)

# Extract model
final_xgb_model <- extract_fit_parsnip(final_fit)



winsorize_x <- function(x, cuts = c(0.01, 0.01)) {
  upper <- quantile(x, 1 - cuts[2], na.rm = TRUE)
  lower <- quantile(x, cuts[1], na.rm = TRUE)
  x[x > upper] <- upper
  x[x < lower] <- lower
  return(x)
}

x_baked <- extract_mold(final_fit)$predictors %>%
  mutate(across(everything(), ~ winsorize_x(.x, c(0.02, 0.02))))

# Reuse preprocessed training data
x_baked <- extract_mold(final_fit)$predictors %>%
  mutate(across(everything(), ~ winsorize_x(.x, c(0.02, 0.02))))

y_baked <- extract_mold(final_fit)$outcomes

# Create DALEX explainer
explainer_xgb <- explain_tidymodels(
  model = final_xgb_model,
  data  = x_baked,
  y     = y_baked,
  label = "XGBoost Exit Model"
)

```

```{r plot-dalex-pdp-top6, echo=FALSE, message=FALSE, warning=FALSE}
xgb_pdp_data <- model_profile(
  explainer_xgb,
  N = 5000,
  variables = c(
    "max_non_interacting_streak",
    "total_sku_type_2_in_product_spacebrand_ID",
    "total_sku_in_product_space",
    "NO_type_2_activated_in_product_spacebrand_ID",
    "total_sku_type_2_in_product_space",
    "click_quantile_overall"
  )
)

plot(xgb_pdp_data, geom = "aggregates", facet_ncol = 3) +
  ggtitle("Partial Dependence Profiles for Top Predictors (XGBoost)") +
  theme_minimal(base_size = 11)
```

These plots demonstrate how the predicted probability of exit changes with key features:

-   `click_quantile_overall`: SKUs with higher past visibility exhibit lower exit probability.
-   `max_non_interacting_streak`: Exit risk rises rapidly after several days of inactivity.
-   `total_sku_type_2_in_product_space`: Local crowding intensifies exit likelihood.
-   `NO_type_2_activated_in_product_spacebrand_ID`: Entry of similar SKUs increases competition and risk.

## 5. Partial Dependence Plot - order_quantile_product

```{r  warning=FALSE}
# 2. Partial Dependence Plot (adjust variable name if needed)
library(ingredients)

pdp_plot <- partial_dependency(xgb_explainer, variables = "order_quantile_product")

plot(pdp_plot) +
  ggtitle("Partial Dependence Plot - order_quantile_product") +
  theme_minimal(base_size = 12)
```

## Key Drivers of Exit Predictions

Based on permutation-based variable importance analysis, we identify the following key insights:

### **Top Drivers of Exit**

-   **`max_non_interacting_streak`** is the most influential predictor.\
    This is intuitive: SKUs with prolonged periods of no interaction (no clicks or purchases) are significantly more likely to be deactivated.

-   **Competitive pressure features** such as **`total_sku_type_2_in_product_space`**, **`total_sku_in_product_space`**, and **`NO_type_2_activated_in_product_spacebrand_ID`** rank highly.\
    These variables reflect **competitive intensity** within the product space and brand group, suggesting that crowd-out effects play a critical role in exit behavior.

### **Relative Importance of Attributes**

-   Dummy variables for **`attribute1`** and **`attribute2`** (e.g., `attribute1_X4`, `attribute2_X70`) are consistently **lower-ranked**.\
    This suggests that **categorical product characteristics are less influential** than features that capture a SKU's relative position in the platform’s product ecosystem.

### **Quantile-Based Features**

-   Features like **`click_quantile_overall`**, **`order_quantile_overall`**, and **`price_quantile_overall`** show **moderate importance**.\
    These quantiles capture **demand-side positioning**, indicating that while consumer interest matters, it is **less predictive** than interaction streaks and competitive saturation.

### **Uncertainty and Stability**

-   The **width of the boxplots** in the variable importance chart reflects **uncertainty across permutations**:
    -   **Narrow boxplots** (e.g., for top-ranked features) indicate **stable, consistently high importance**.
    -   **Wider boxplots** suggest **less certainty** and potential variation in importance across different data resamples.

```{r get-top-numeric-vars, echo=FALSE, include=FALSE, message=FALSE, warning=FALSE}
library(DALEXtra)
library(ingredients)
library(dplyr)
library(ggforce)  # For paginated plots


# Get valid (numerical) column names from the explainer's data
valid_vars <- xgb_explainer$data %>%
  select(where(is.numeric)) %>%  # Keep only numeric variables
  colnames()

top_numeric_vars <- vi_dalex %>%
  filter(variable != "_full_model_", variable %in% valid_vars) %>%
  distinct(variable, .keep_all = TRUE) %>%
  arrange(desc(dropout_loss)) %>%
  slice_head(n = 6) %>%
  pull(variable)


print(top_numeric_vars)  # Sanity check



```

```{r compute-shap-values, echo=FALSE, include=FALSE, message=FALSE, warning=FALSE}
xgb_booster <- extract_fit_parsnip(final_fit)$fit


predict_wrapper <- function(object, newdata) {
  newdata_mat <- as.matrix(newdata)
  predict(object, newdata = newdata_mat)
}



library(fastshap)
library(doFuture)
registerDoFuture()
plan(multisession, workers = parallel::detectCores() - 1)

set.seed(1234)
shap_values <- fastshap::explain(
  object = xgb_booster,
  X = x_baked,
  pred_wrapper = predict_wrapper,
  nsim = 50,
  parallel = TRUE,
  .export = "predict_wrapper"
)

```

## 8. SHAP Dependence Plot: max_non_interacting_streak

Illustrates how the impact of a single feature varies across SKUs and interacts with other features.

```{r  plot-shap-global-and-dependence, echo=FALSE, message=FALSE, warning=FALSE}
library(shapviz)

# SHAP object
shv <- shapviz(shap_values, X = x_baked)

# Global importance plot
sv_importance(shv, kind = "beeswarm")

# Dependence plot for most important variable
sv_dependence(shv, v = "max_non_interacting_streak")

```

## SHAP-Based Model Explanation

We use SHAP (SHapley Additive exPlanations) values to understand the marginal effect of each feature on the XGBoost model’s predicted probability of SKU exit. These visualizations help interpret both global feature importance and local variable interactions.

### **1. Global SHAP Importance Summary (Beeswarm Plot)**

The SHAP beeswarm plot reveals the most influential features across all predictions:

-   **`max_non_interacting_streak`** dominates as the top predictor.\
    SKUs with prolonged periods of inactivity (no clicks or orders) consistently show **high SHAP values**, indicating strong contribution to **increased exit probability**.

-   Features like **`click_quantile_overall`**, **`total_sku_type_2_in_product_spacebrand_ID`**, and **`NO_type_2_activated_in_product_spacebrand_ID`** also rank highly.\
    These capture both **demand-side activity** and **competitive crowding**, suggesting that SKUs in saturated environments with declining engagement are most vulnerable.

-   **Color gradients** in the beeswarm plot show how high values (e.g., longer streaks or high competition) align with SHAP shifts toward predicted exits.

### **2. SHAP Dependence Plot: `max_non_interacting_streak` × `click_quantile_overall`**

This plot illustrates how **interaction between variables** impacts predictions:

-   The SHAP value for **`max_non_interacting_streak`** increases with its magnitude, reflecting a **rising risk of exit** as inactivity stretches.

-   Even SKUs with **high historical visibility** (e.g., high `click_quantile_overall`, colored orange) exhibit **elevated exit risk** when recent interaction streaks are long.

-   This highlights a key insight: **prior visibility is not protective** if user engagement disappears for an extended period.

### **3. Interpretation Summary**

These SHAP-based insights affirm that:

-   **Sustained user engagement is the strongest driver of survival.**\
    The model detects that **disengagement overrides past popularity**, signaling risk even for once-popular SKUs.

-   **Competitive intensity and product-space crowding** have **moderate but meaningful influence**, reinforcing the idea that platform dynamics—such as over-saturation—also shape seller success.

-   Together, the plots suggest that the **model prioritizes recent behavior patterns over static or historical characteristics**, which aligns with the real-time, dynamic nature of e-commerce platforms.

```{r create-xgb-explainer, echo=FALSE, include=FALSE, message=FALSE, warning=FALSE}
# Already done: extract model, prepare x_baked and y_baked
final_xgb_model <- extract_fit_parsnip(final_fit)

xgb_explainer <- explain_tidymodels(
  model = final_xgb_model,
  data = baked_train %>% select(-exited),
  y = numeric_exited,
  label = "XGBoost Exit Model"
)


```

## 9. Local SHAP Waterfall Plot (Single SKU)

Breaks down the prediction for one test observation step-by-step.

```{r plot-local-breakdown, echo=FALSE, message=FALSE, warning=FALSE}

 #Step 1: Pick 1st test observation 
example_obs <- baked_test[1, , drop = FALSE]



library(DALEX)

local_expl <- predict_parts(
  explainer = xgb_explainer,
  new_observation = example_obs,
  type = "break_down"
)

plot(local_expl) +
  ggtitle("Break-down Plot: Why Did This SKU Get Predicted as Exit?")

```

### **Local SHAP Explanation: Waterfall Plot for a Single SKU**

This SHAP waterfall plot illustrates how **individual feature values contribute** to the model’s predicted probability of exit for one specific SKU:

-   The **intercept (baseline prediction)** is **0.012**, which reflects the average predicted probability of exit across the training set.

-   The SKU has **`click_quantile_overall = 9`** and **`max_non_interacting_streak = 26`**, both of which slightly **reduce the predicted probability** compared to the baseline.

-   Other features, such as **`total_sku_type_2_in_product_space`**, contribute minimally or have **near-zero impact**.

-   The **final predicted probability is 0.002**, suggesting a **very low likelihood of exit** for this SKU.

#### **🔍 Key Insight**

> **Despite some moderate risk signals** (like a long streak of non-interaction), the combination of other feature values—especially **high historical engagement**—drives the prediction toward **continued activity** rather than exit.

This aligns with our earlier findings: while disengagement is critical, **strong prior visibility can buffer against exit**, at least in specific SKU cases. ##10. SHAP Force Plot (Single SKU)

Visualizes how feature effects accumulate from the base prediction.

```{r plot-shap-force-local, echo=FALSE, message=FALSE, warning=FALSE}
sv_force(shv, row_id = 1)  # assuming `shv` was created using shapviz(shap_values, X = x_baked)

```

### **Local SHAP Explanation: Step-by-Step Contribution Plot**

This plot shows how **SHAP values accumulate** from the model’s baseline to the final prediction for a given SKU:

-   The **baseline prediction** (intercept) starts at **0.000**.

-   Features such as **`max_non_interacting_streak = 12`**, **`total_sku_type_2_in_product_spacebrand_ID = 184`**, and **`attribute2_Missing = 0`** each contribute **positively** to the predicted exit probability.

-   A group of **25 other features** has a **slightly negative cumulative effect**, pulling the prediction down marginally.

-   The **final prediction is approximately 0.00695**, which remains **low**, indicating a **low exit likelihood** for this SKU.

#### **🔍 Key Insight**

> The SHAP plot offers **granular visibility** into how each individual feature **nudges the prediction** either upward or downward. Even when several variables contribute to risk, the **combined effect may still indicate low probability of exit** if no strong negative drivers dominate.

```{r plot-pdp-top6-ingredients, echo=FALSE, message=FALSE, warning=FALSE}
# 
# 
# # The DALEX-style PDP plots above used model_profile(variables = NULL) which runs for all predictors.
# # Here, we recreate the PDP step using ingredients::partial_dependency on top 6 variables.
# 
# library(ingredients)
# library(ggforce)
# 
# # Identify top numeric variables from variable importance output
# valid_vars <- x_baked %>%
#   select(where(is.numeric)) %>%
#   colnames()
# 
# top_numeric_vars <- vip_xgb %>%
#   filter(variable != "_full_model_", variable %in% valid_vars) %>%
#   distinct(variable, .keep_all = TRUE) %>%
#   arrange(desc(dropout_loss)) %>%
#   slice_head(n = 6) %>%
#   pull(variable)
# 
# # Generate PDPs for selected variables (paginated if needed)
# xgb_pdp_data <- partial_dependency(
#   explainer = explainer_xgb,
#   variables = top_numeric_vars,
#   N = 5000,
#   grid_points = 20
# )
# 
# # Plot all PDPs with pagination
# plot(xgb_pdp_data, geom = "aggregates", facet_ncol = 2) +
#   ggtitle("Partial Dependence Plots for Top 6 Variables") +
#   theme_minimal(base_size = 12)

```

#Refrence

@article{chi2022competition, title = {Competition or spillover? Effects of platform-owner entry on provider commitment}, author = {Chi, Y. and Qing, P. and Jin, Y. J. and Yu, J. and Dong, M. C. and Huang, L.}, journal = {Journal of Business Research}, volume = {144}, pages = {627--636}, year = {2022} }

@misc{crawford2022amazon, title = {Amazon Entry on Amazon Marketplace}, author = {Crawford, G. S. and Courthoud, M. and Seibel, R. and Zuzek, S.}, year = {2022}, note = {Working paper} }

@article{deng2023can, title = {Can third-party sellers benefit from a platform’s entry to the market?}, author = {Deng, Y. and Tang, C. S. and Wang, W. and Yoo, O. S.}, journal = {Service Science}, volume = {15}, number = {4}, pages = {233--249}, year = {2023} }

@article{hu2024supercharged, title = {Supercharged by Advanced Analytics, JD.com Attains Agility, Resilience, and Shared Value Across Its Supply Chain}, author = {Hu, H. and Qi, Y. and Lee, H. L. and Shen, Z.-J. M. and Liu, C. and Zhu, W. and Kang, N.}, journal = {INFORMS Journal on Applied Analytics}, volume = {54}, number = {1}, pages = {54--70}, year = {2024} }

@article{jiang2011firm, title = {Firm strategies in the “mid tail” of platform-based retailing}, author = {Jiang, B. and Jerath, K. and Srinivasan, K.}, journal = {Marketing Science}, volume = {30}, number = {5}, pages = {757--775}, year = {2011} }

@article{shen2024jd, title = {JD.com: Transaction-level data for the 2020 MSOM data driven research challenge}, author = {Shen, M. and Tang, C. S. and Wu, D. and Yuan, R. and Zhou, W.}, journal = {Manufacturing & Service Operations Management}, volume = {26}, number = {1}, pages = {2--10}, year = {2024} }

@article{song2020spillover, title = {Spillover effect of consumer awareness on third parties’ selling strategies and retailers’ platform openness}, author = {Song, W. and Chen, J. and Li, W.}, journal = {Information Systems Research}, volume = {32}, number = {1}, pages = {172--193}, year = {2020} }

@article{zhu2018competing, title = {Competing with complementors: An empirical look at Amazon.com}, author = {Zhu, F. and Liu, Q.}, journal = {Strategic Management Journal}, volume = {39}, number = {10}, pages = {2618--2642}, year = {2018} }
